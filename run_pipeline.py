import os
import sys
import sqlite3
import subprocess
from pathlib import Path
from typing import List, Tuple

# ========= CONFIG =========
CRAWL_YEAR = int(os.getenv("CRAWL_YEAR", "2025"))
PIPELINE_LIMIT = int(os.getenv("PIPELINE_LIMIT", "5"))

ROOT = Path(__file__).resolve().parent
DB_PATH = ROOT / "data.db"
OUTPUT_DIR = ROOT / "outputs"

DOWNLOADER = ROOT / "download_pdf.py"  # script ·ªü root

def run_cmd(args: list[str], env=None, title: str = ""):
    """Ch·∫°y l·ªánh b·∫±ng interpreter hi·ªán t·∫°i, CWD=repo root, in log ƒë·∫ßy ƒë·ªß."""
    if title:
        print(title)
    # ƒë·∫£m b·∫£o Python nh√¨n th·∫•y package 'src'
    env2 = os.environ.copy()
    if env:
        env2.update(env)
    env2["PYTHONPATH"] = (
        f"{ROOT}:{env2.get('PYTHONPATH','')}"
        if sys.platform != "win32"
        else f"{ROOT};{env2.get('PYTHONPATH','')}"
    )
    proc = subprocess.run(
        [sys.executable] + args,
        cwd=str(ROOT),
        env=env2,
        text=True,
        capture_output=True,
    )
    if proc.stdout:
        print(proc.stdout)
    if proc.returncode != 0:
        if proc.stderr:
            print(proc.stderr)
        raise subprocess.CalledProcessError(proc.returncode, args)
    return proc

def fetch_personal_links(year: int, limit: int) -> List[Tuple[str, str]]:
    """L·∫•y (title, url) c·ªßa link 'personal' theo nƒÉm t·ª´ SQLite."""
    if not DB_PATH.exists():
        return []
    conn = sqlite3.connect(str(DB_PATH))
    try:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT title, url
            FROM links
            WHERE year = ? AND bucket = 'personal'
            ORDER BY id DESC
            LIMIT ?
            """,
            (year, limit),
        )
        return [(r[0], r[1]) for r in cur.fetchall()]
    finally:
        conn.close()

# 1) Crawl -> ghi SQLite
try:
    run_cmd(["-m", "src.crawl_links_and_classify"], title="üöÄ ƒêang crawl link m·ªõi b·∫±ng Playwright (ghi v√†o SQLite)‚Ä¶")
except subprocess.CalledProcessError:
    print("‚ö†Ô∏è Crawler l·ªói, ti·∫øp t·ª•c pipeline‚Ä¶")

# 2) L·∫•y link personal nƒÉm 2025
links = fetch_personal_links(CRAWL_YEAR, PIPELINE_LIMIT)
print(f"üîó S·∫Ω x·ª≠ l√Ω {len(links)} link 'C√° nh√¢n' (nƒÉm {CRAWL_YEAR})")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 3) Download -> OCR + parse + upsert DB
for idx, (title, url) in enumerate(links, 1):
    print(f"\nüü° [{idx}] {title}")
    try:
        # t·∫£i PDF
        run_cmd([str(DOWNLOADER), url], title="‚¨áÔ∏è  ƒêang t·∫£i PDF‚Ä¶")

        # truy·ªÅn ngu·ªìn cho extractor
        env = {"CURRENT_SOURCE_URL": url}

        # OCR + parse + ghi DB (Document AI d√πng trong extract_to_db.py)
        run_cmd(["-m", "src.extract_to_db"], env=env, title="üß†  OCR & parse & ghi DB‚Ä¶")

        # d·ªçn file PDF c√≤n s√≥t (extractor ƒë√£ t·ª± x√≥a khi th√†nh c√¥ng)
        for f in OUTPUT_DIR.glob("*.pdf"):
            try:
                f.unlink(missing_ok=True)
            except Exception:
                pass

        print("‚úÖ Ho√†n t·∫•t 1 link.")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω: {e}")

print("\nüéâ Pipeline k·∫øt th√∫c.")
