import os
import sys
import sqlite3
import subprocess
from pathlib import Path
from typing import List, Tuple

# ========= CONFIG =========
CRAWL_YEAR = int(os.getenv("CRAWL_YEAR", "2025"))
PIPELINE_LIMIT = int(os.getenv("PIPELINE_LIMIT", "5"))

ROOT = Path(__file__).resolve().parent
DB_PATH = ROOT / "data.db"
OUTPUT_DIR = ROOT / "outputs"

# ƒê∆∞·ªùng d·∫´n script
CRAWLER    = ROOT / "src" / "crawl_links_and_classify.py"  # trong src
DOWNLOADER = ROOT / "download_pdf.py"                       # ·ªü root
EXTRACTOR  = ROOT / "src" / "extract_to_db.py"              # trong src

def run_cmd(script: Path | str, args: list[str] | None = None, env=None, title: str = ""):
    """Ch·∫°y 1 script Python v·ªõi c√πng interpreter, CWD=repo root; in log ƒë·∫ßy ƒë·ªß."""
    if title:
        print(title)
    cmd = [sys.executable, str(script)]
    if args:
        cmd.extend(args)
    proc = subprocess.run(
        cmd,
        cwd=str(ROOT),
        env=env or os.environ.copy(),
        text=True,
        capture_output=True,
    )
    if proc.stdout:
        print(proc.stdout)
    if proc.returncode != 0:
        if proc.stderr:
            print(proc.stderr)
        raise subprocess.CalledProcessError(proc.returncode, cmd)
    return proc

def fetch_personal_links(year: int, limit: int) -> List[Tuple[str, str]]:
    """L·∫•y (title, url) c·ªßa link 'personal' theo nƒÉm t·ª´ SQLite."""
    if not DB_PATH.exists():
        return []
    conn = sqlite3.connect(str(DB_PATH))
    try:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT title, url
            FROM links
            WHERE year = ? AND bucket = 'personal'
            ORDER BY id DESC
            LIMIT ?
            """,
            (year, limit),
        )
        return [(r[0], r[1]) for r in cur.fetchall()]
    finally:
        conn.close()

# 1) Crawl -> ghi SQLite
try:
    run_cmd(CRAWLER, title="üöÄ ƒêang crawl link m·ªõi b·∫±ng Playwright (ghi v√†o SQLite)‚Ä¶")
except subprocess.CalledProcessError:
    print("‚ö†Ô∏è Crawler l·ªói, ti·∫øp t·ª•c pipeline‚Ä¶")

# 2) L·∫•y link personal nƒÉm 2025
links = fetch_personal_links(CRAWL_YEAR, PIPELINE_LIMIT)
print(f"üîó S·∫Ω x·ª≠ l√Ω {len(links)} link 'C√° nh√¢n' (nƒÉm {CRAWL_YEAR})")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 3) Download -> OCR + parse + upsert DB
for idx, (title, url) in enumerate(links, 1):
    print(f"\nüü° [{idx}] {title}")
    try:
        # T·∫£i PDF
        run_cmd(DOWNLOADER, [url], title="‚¨áÔ∏è  ƒêang t·∫£i PDF‚Ä¶")

        # Truy·ªÅn ngu·ªìn cho extractor ƒë·ªÉ ghi v√†o DB
        env = os.environ.copy()
        env["CURRENT_SOURCE_URL"] = url

        # OCR + parse + ghi DB (Google Document AI ƒë∆∞·ª£c d√πng trong extract_to_db.py)
        run_cmd(EXTRACTOR, env=env, title="üß†  OCR & parse & ghi DB‚Ä¶")

        # D·ªçn file PDF c√≤n s√≥t (extractor ƒë√£ x√≥a khi th√†nh c√¥ng)
        for f in OUTPUT_DIR.glob("*.pdf"):
            try:
                f.unlink(missing_ok=True)
            except Exception:
                pass

        print("‚úÖ Ho√†n t·∫•t 1 link.")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω: {e}")

print("\nüéâ Pipeline k·∫øt th√∫c.")
